{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Synthesis waveform from trained model.\n",
    "\n",
    "usage: synthesis.py [options] <checkpoint> <text_list_file> <dst_dir>\n",
    "\n",
    "options:\n",
    "    --hparams=<parmas>                Hyper parameters [default: ].\n",
    "    --checkpoint-seq2seq=<path>       Load seq2seq model from checkpoint path.\n",
    "    --checkpoint-postnet=<path>       Load postnet model from checkpoint path.\n",
    "    --file-name-suffix=<s>            File name suffix [default: ].\n",
    "    --max-decoder-steps=<N>           Max decoder steps [default: 500].\n",
    "    --replace_pronunciation_prob=<N>  Prob [default: 0.0].\n",
    "    --speaker_id=<id>                 Speaker ID (for multi-speaker model).\n",
    "    --output-html                     Output html for blog post.\n",
    "    -h, --help               Show help message.\n",
    "\"\"\"\n",
    "from docopt import docopt\n",
    "import sys\n",
    "import os\n",
    "from os.path import dirname, join, basename, splitext\n",
    "import audio\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import nltk\n",
    "# The deepvoice3 model\n",
    "from deepvoice3_pytorch import frontend\n",
    "from hparams import hparams\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "_frontend = None  # to be set later\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tts(model, text, p=0, speaker_id=None, fast=False):\n",
    "    \"\"\"Convert text to speech waveform given a deepvoice3 model.\n",
    "\n",
    "    Args:\n",
    "        text (str) : Input text to be synthesized\n",
    "        p (float) : Replace word to pronounciation if p > 0. Default is 0.\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    if fast:\n",
    "        model.make_generation_fast_()\n",
    "\n",
    "    sequence = np.array(_frontend.text_to_sequence(text, p=p))\n",
    "    sequence = Variable(torch.from_numpy(sequence)).unsqueeze(0)\n",
    "    text_positions = torch.arange(1, sequence.size(-1) + 1).unsqueeze(0).long()\n",
    "    text_positions = Variable(text_positions)\n",
    "    speaker_ids = None if speaker_id is None else Variable(torch.LongTensor([speaker_id]))\n",
    "    if use_cuda:\n",
    "        sequence = sequence.cuda()\n",
    "        text_positions = text_positions.cuda()\n",
    "        speaker_ids = None if speaker_ids is None else speaker_ids.cuda()\n",
    "\n",
    "    # Greedy decoding\n",
    "    mel_outputs, linear_outputs, alignments, done = model(\n",
    "        sequence, text_positions=text_positions, speaker_ids=speaker_ids)\n",
    "\n",
    "    linear_output = linear_outputs[0].cpu().data.numpy()\n",
    "    spectrogram = audio._denormalize(linear_output)\n",
    "    alignment = alignments[0].cpu().data.numpy()\n",
    "    mel = mel_outputs[0].cpu().data.numpy()\n",
    "    mel = audio._denormalize(mel)\n",
    "\n",
    "    # Predicted audio signal\n",
    "    waveform = audio.inv_spectrogram(linear_output.T)\n",
    "\n",
    "    return waveform, alignment, spectrogram, mel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'test.pickle', 'rb') as file2:\n",
    "    args = pickle.load(file2)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec2numpy(sec):\n",
    "    return np.zeros(int(24000*sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.parse(args[\"--hparams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Command line args:\\n\", args)\n",
    "checkpoint_path = args[\"<checkpoint>\"]\n",
    "text_list_file_path = args[\"<text_list_file>\"]\n",
    "dst_dir = args[\"<dst_dir>\"]\n",
    "checkpoint_seq2seq_path = args[\"--checkpoint-seq2seq\"]\n",
    "checkpoint_postnet_path = args[\"--checkpoint-postnet\"]\n",
    "max_decoder_steps = int(args[\"--max-decoder-steps\"])\n",
    "file_name_suffix = args[\"--file-name-suffix\"]\n",
    "replace_pronunciation_prob = float(args[\"--replace_pronunciation_prob\"])\n",
    "#output_html = args[\"--output-html\"]\n",
    "speaker_id = args[\"--speaker_id\"]\n",
    "#if speaker_id is not None:\n",
    "#    speaker_id = int(speaker_id)\n",
    "\n",
    "# Override hyper parameters\n",
    "hparams.parse(args[\"--hparams\"])\n",
    "assert hparams.name == \"deepvoice3\"\n",
    "\n",
    "# Presets\n",
    "if hparams.preset is not None and hparams.preset != \"\":\n",
    "    preset = hparams.presets[hparams.preset]\n",
    "    import json\n",
    "    hparams.parse_json(json.dumps(preset))\n",
    "    print(\"Override hyper parameters with preset \\\"{}\\\": {}\".format(\n",
    "        hparams.preset, json.dumps(preset, indent=4)))\n",
    "\n",
    "_frontend = getattr(frontend, hparams.frontend)\n",
    "import train\n",
    "train._frontend = _frontend\n",
    "from train import plot_alignment, build_model\n",
    "\n",
    "# Model\n",
    "model = build_model()\n",
    "\n",
    "# Load checkpoints separately\n",
    "if checkpoint_postnet_path is not None and checkpoint_seq2seq_path is not None:\n",
    "    checkpoint = torch.load(checkpoint_seq2seq_path)\n",
    "    model.seq2seq.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    checkpoint = torch.load(checkpoint_postnet_path)\n",
    "    model.postnet.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    checkpoint_name = splitext(basename(checkpoint_seq2seq_path))[0]\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    checkpoint_name = splitext(basename(checkpoint_path))[0]\n",
    "\n",
    "model.seq2seq.decoder.max_decoder_steps = max_decoder_steps\n",
    "\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "with open(text_list_file_path, \"rb\") as f:\n",
    "    lines = f.readlines()\n",
    "    au1 = np.array([])\n",
    "    for idx, line in enumerate(lines):\n",
    "        text = line.decode(\"utf-8\")[:-1]\n",
    "        words = nltk.word_tokenize(text)\n",
    "        waveform, alignment, _, _ = tts(model, text, p=replace_pronunciation_prob, speaker_id=speaker_id, fast=True)\n",
    "        dst_wav_path = join(dst_dir, \"{}_{}{}.wav\".format(idx, checkpoint_name, file_name_suffix))\n",
    "        dst_alignment_path = join(dst_dir, \"{}_{}{}_alignment.png\".format(idx, checkpoint_name,file_name_suffix))\n",
    "        plot_alignment(alignment.T, dst_alignment_path,\n",
    "                                                    info=\"{}, {}\".format(hparams.builder, basename(checkpoint_path)))\n",
    "        '''\n",
    "        audio. (waveform, dst_wav_path)\n",
    "        from os.path import basename, splitext\n",
    "        name = splitext(basename(text_list_file_path))[0]\n",
    "        print(name)\n",
    "        '''\n",
    "        au1 = waveform  if idx == 0 else np.concatenate((au1,sec2numpy(0.1), waveform),axis=0)\n",
    "    ipd.Audio(au1, rate=hparams.sample_rate)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ipd.Audio(au1, rate=hparams.sample_rate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('base': conda)",
   "language": "python",
   "name": "python36964bitbaseconda162ed9b61071405a8894c3373bccd2ea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
